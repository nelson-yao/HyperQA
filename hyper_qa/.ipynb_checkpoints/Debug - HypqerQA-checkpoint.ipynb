{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from hyper_qa.model import HyperQA\n",
    "from hyper_qa.utilities import riemannian_gradient\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_random_seed(212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.normal(0, 1, (1000, 300))\n",
    "def generate_samples(max_length, sample_size, vocab_size=1000, pad=False):\n",
    "    lengths = []\n",
    "    sample_sequences = []\n",
    "    for i in range(sample_size):\n",
    "        text = np.random.randint(1,1000, np.random.randint(1,max_length))\n",
    "#         text = np.arange(i%max_length)\n",
    "        if pad:\n",
    "            sample = np.concatenate((text, [0] * (max_length - len(text))))\n",
    "        else:\n",
    "            sample = text\n",
    "        lengths.append(len(text))\n",
    "        sample_sequences.append(sample)\n",
    "    if pad:\n",
    "        sample_sequences = np.vstack(sample_sequences)\n",
    "    return sample_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "q = generate_samples(max_length, 200, pad=True)\n",
    "a = generate_samples(max_length, 200, pad=True)\n",
    "b = generate_samples(max_length, 200, pad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((q, a, b))\n",
    "dataset = dataset.shuffle(200).batch(5)\n",
    "data_iter = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, a, b = data_iter.get_next()\n",
    "model = HyperQA(1000, max_length, embedding_matrix=embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = tf.constant(5.0)\n",
    "loss_history = []\n",
    "steps = 500\n",
    "for q1, q2, q3 in data_iter:\n",
    "    with tf.GradientTape() as tape:\n",
    "        sim_pos, sim_neg = model((q,a,b), training=True)\n",
    "        loss = tf.reduce_mean(tf.maximum(0.0, margin + sim_neg - sim_pos))\n",
    "        loss_history.append(loss)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    riem_gradients = [gradients[0]] + [riemannian_gradient(grad) for grad in gradients[1:]]\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999996  0.99999994 1.         1.         1.         1.0000001\n",
      " 0.9999995  0.99999976 0.99999994 1.         0.99999964 1.\n",
      " 1.0000001  0.99999994 0.99999976 0.99999976 0.99999994 0.99999976\n",
      " 1.         0.99999994 0.99999994 0.9999994  0.99999964 0.9999995\n",
      " 1.0000001  0.9999997  0.9999998  0.99999994 1.0000004  1.\n",
      " 0.9999994  1.         1.0000002  0.99999994 0.99999946 1.0000001\n",
      " 1.         1.0000001  0.99999994 0.9999995  0.99999994 1.\n",
      " 0.9999996  1.         1.0000001  1.0000004  0.99999994 0.9999998\n",
      " 0.9999994  0.99999994 0.9999997  0.99999976 1.         1.\n",
      " 0.9999999  0.9999997  0.99999994 0.99999994 0.9999996  0.99999994\n",
      " 0.99999994 0.9999998  1.0000001  0.9999999  1.0000001  1.0000002\n",
      " 0.99999994 0.9999999  0.9999998  1.         1.         0.99999946\n",
      " 0.9999997  1.0000001  0.9999997  1.0000001  1.0000001  0.9999998\n",
      " 0.99999946 0.99999994 0.9999994  0.99999994 0.99999994 1.0000001\n",
      " 0.9999995  0.99999994 0.9999994  1.0000001  0.9999996  1.\n",
      " 0.9999995  1.0000001  0.99999964 1.         1.         0.99999976\n",
      " 0.99999964 0.99999994 1.0000001  1.         0.99999964 0.99999994\n",
      " 1.0000001  0.9999999  0.9999999  1.         0.99999994 1.0000002\n",
      " 0.99999994 1.         0.99999964 0.99999946 0.9999996  1.\n",
      " 1.0000001  1.         0.99999994 0.9999999  1.         1.0000002\n",
      " 1.0000002  0.9999995  0.99999994 1.         1.0000001  1.\n",
      " 1.0000001  0.9999999  0.99999994 1.         0.99999994 0.99999964\n",
      " 0.99999976 1.0000001  0.99999994 1.         0.9999995  0.99999976\n",
      " 0.99999994 0.99999994 0.9999998  1.         1.         0.99999994\n",
      " 0.99999994 0.99999994 0.9999998  0.9999995  0.99999976 0.9999994\n",
      " 1.         1.         0.99999994 0.99999964 1.         1.\n",
      " 1.0000001  1.0000001  0.99999994 0.9999997  0.99999994 0.99999994\n",
      " 1.0000002  0.99999994 0.9999999  1.0000002  0.99999994 1.\n",
      " 0.99999994 0.99999946 1.         0.99999994 1.0000001  0.9999997\n",
      " 0.9999997  0.9999997  1.         0.9999998  1.0000004  0.99999994\n",
      " 0.9999999  1.         0.99999994 1.0000001  1.         0.99999994\n",
      " 0.99999946 0.99999964 1.0000002  0.99999994 1.0000002  1.\n",
      " 0.9999996  1.         1.         1.0000002  1.0000002  1.\n",
      " 0.99999976 1.0000001 ]\n",
      "[0.99999994 1.         0.99999994 0.99999964 1.         1.0000001\n",
      " 1.         0.99999994 0.9999995  1.0000001  1.         0.99999994\n",
      " 0.99999994 1.         0.99999994 0.99999994 0.99999994 0.99999994\n",
      " 1.0000002  1.0000001  1.0000001  1.         1.         0.9999995\n",
      " 1.0000001  1.0000002  1.0000002  0.99999994 1.         1.0000002\n",
      " 0.9999999  0.99999964 1.         1.         1.0000001  0.9999999\n",
      " 0.99999994 0.9999999  0.99999994 0.99999994 0.9999999  1.\n",
      " 0.9999993  0.99999994 1.0000001  0.9999999  0.99999994 0.99999994\n",
      " 0.9999997  0.9999995  1.         0.99999994 0.99999994 1.\n",
      " 0.99999994 0.9999995  1.0000002  1.0000001  1.         0.99999994\n",
      " 1.         0.99999994 0.99999994 0.99999994 1.0000002  0.99999994\n",
      " 0.9999996  0.99999994 1.         0.9999997  1.         1.0000001\n",
      " 0.99999994 0.9999997  1.         0.99999994 0.9999997  0.99999994\n",
      " 1.         0.9999997  1.         0.99999946 1.         0.99999946\n",
      " 0.99999994 1.0000001  1.0000001  1.         0.9999998  1.0000001\n",
      " 1.         1.0000001  0.9999996  0.99999994 1.0000004  0.9999995\n",
      " 1.         1.0000001  0.99999994 1.         0.99999994 0.9999997\n",
      " 1.         1.         0.99999994 0.99999994 0.99999994 0.99999994\n",
      " 0.9999998  1.         1.         0.9999998  1.         1.\n",
      " 0.99999994 1.         1.0000004  0.9999998  1.         1.0000002\n",
      " 0.99999964 1.0000001  0.9999999  1.0000004  0.99999994 1.\n",
      " 0.99999964 1.         0.99999994 0.99999994 1.         0.99999964\n",
      " 0.99999964 0.99999976 1.0000001  0.99999964 1.         0.99999964\n",
      " 1.0000004  1.         0.99999994 0.99999994 1.0000002  0.99999946\n",
      " 0.99999994 0.9999995  0.9999996  1.0000001  0.99999994 0.99999994\n",
      " 1.         1.         0.9999996  1.0000001  1.         0.99999994\n",
      " 0.99999946 0.9999999  1.         1.0000001  0.9999997  1.0000001\n",
      " 1.         0.9999995  0.99999976 1.         0.99999994 0.9999999\n",
      " 1.0000001  1.0000001  0.99999994 1.0000001  1.         0.99999964\n",
      " 1.0000001  0.9999999  0.99999994 1.         0.99999994 0.99999994\n",
      " 0.99999964 1.0000001  1.0000002  1.         0.9999997  0.99999994\n",
      " 1.0000001  0.99999994 1.0000001  1.         1.         0.99999946\n",
      " 1.         0.99999976 1.0000001  0.9999998  1.         1.0000002\n",
      " 1.0000001  0.99999976]\n",
      "[0.9999995  1.         0.99999994 1.         0.99999994 1.0000002\n",
      " 1.         1.         0.99999994 1.0000001  1.0000002  0.9999995\n",
      " 0.99999964 0.99999994 0.99999994 1.0000001  0.99999994 0.99999994\n",
      " 0.99999994 0.99999994 1.         1.         1.0000002  0.9999997\n",
      " 0.9999994  0.99999994 0.9999997  0.9999999  0.99999994 0.99999994\n",
      " 1.         0.99999994 0.99999994 0.9999996  1.0000001  1.0000002\n",
      " 1.0000002  0.99999994 0.9999993  0.9999996  0.99999994 1.\n",
      " 1.         0.9999995  1.0000001  1.0000001  0.9999995  1.0000001\n",
      " 0.99999994 1.0000002  0.9999998  0.99999994 1.         1.\n",
      " 1.0000001  0.9999996  1.0000001  1.0000001  0.99999994 1.0000001\n",
      " 0.9999998  1.         0.99999994 1.0000001  0.99999994 0.99999994\n",
      " 1.         0.9999998  1.         0.9999995  1.0000002  1.0000002\n",
      " 0.9999994  0.99999994 0.99999964 0.9999996  1.         0.9999995\n",
      " 0.99999994 0.9999999  0.99999994 0.99999976 1.         0.9999999\n",
      " 0.99999994 0.99999946 0.99999994 0.99999964 0.9999996  1.0000001\n",
      " 0.9999999  0.99999994 0.9999999  1.0000002  1.0000002  0.9999995\n",
      " 0.99999994 1.0000004  0.99999976 1.0000002  0.99999994 1.\n",
      " 1.0000001  1.         0.9999998  1.0000001  1.0000002  0.9999999\n",
      " 1.         0.99999994 1.0000004  1.0000001  1.         1.0000001\n",
      " 1.         0.99999964 0.9999998  0.99999994 1.0000001  1.0000001\n",
      " 0.99999994 0.99999994 0.99999994 0.9999999  0.99999994 1.\n",
      " 0.99999994 1.0000002  0.99999976 0.9999998  1.0000001  1.0000001\n",
      " 0.9999995  1.         0.9999997  0.9999999  0.9999998  1.\n",
      " 0.99999994 1.0000001  1.         0.9999994  0.99999994 1.\n",
      " 0.99999964 1.         0.99999994 0.9999994  0.9999998  1.\n",
      " 0.99999994 1.         1.0000001  1.0000001  1.0000002  1.0000001\n",
      " 1.         0.99999994 0.99999994 1.         0.99999994 1.\n",
      " 0.99999994 1.         0.9999994  1.0000002  0.9999997  0.99999994\n",
      " 0.9999999  0.99999994 1.0000001  0.99999994 1.         0.99999994\n",
      " 1.         1.         0.99999994 0.9999997  1.0000001  0.99999994\n",
      " 1.         0.99999994 0.99999976 0.99999994 1.0000001  0.9999999\n",
      " 1.         1.         0.9999998  0.9999995  1.         0.99999994\n",
      " 0.9999996  0.99999994 0.9999999  1.         0.99999946 0.99999994\n",
      " 0.99999964 1.        ]\n",
      "tf.Tensor(\n",
      "[10.818442  11.677022  11.281071  11.520401  11.8302765 11.264727\n",
      " 11.824038  10.68467   11.355925  11.162595  11.372887  10.191732\n",
      " 11.242524  11.415624  11.875577  11.436732   9.149626  11.244441\n",
      " 10.726863  11.488278  11.587285  10.453906  10.255274   9.99666\n",
      " 11.7747755 10.267976  10.898219  11.001721  11.678906  11.498602\n",
      " 11.380758  11.407449  11.348457  11.716929  11.504766  11.299377\n",
      " 11.483173  11.732681  11.182649  11.033006  11.550136  11.156593\n",
      " 11.437992  11.58758   11.525007  11.264615  10.408749  11.408955\n",
      " 11.395667  11.33425   10.6407795 11.274038  11.636385  11.59825\n",
      " 11.215917  11.103457  11.440689  11.419725  10.676672  11.291024\n",
      " 10.485209  11.424623  11.289447  11.721907  11.048085  11.096226\n",
      " 11.175801  10.083193  11.133113  11.67348   11.041031  11.394486\n",
      " 10.309494  11.343068  11.255995  11.18964   11.354797  10.103772\n",
      " 11.561509  10.847623  10.304527  10.636127  11.267264  11.225298\n",
      " 11.270615  11.322274  10.368712  10.524133  10.571001  11.607966\n",
      " 11.207623  10.296909  11.214278  11.3815155 11.153637  11.2450285\n",
      " 11.661977  10.842312  11.36347   11.182609  11.472625  11.561064\n",
      " 11.051494  10.926541   9.479117  11.81163   11.476942  11.020034\n",
      " 11.378805  10.003828  11.797951  11.620597  10.448488  11.159067\n",
      " 11.1590605 11.123901  11.136895  11.592113  11.473692  11.122814\n",
      " 11.734925  11.476227  10.890221  11.022759  11.528275  11.404062\n",
      " 11.4279585 11.3302965 10.354095  10.992923  11.513345  11.640961\n",
      " 11.144779  11.939771  11.258149  10.291296   9.989432  11.447841\n",
      " 11.360142  11.363395  11.556946  11.267086  11.523512  11.348997\n",
      " 11.70189    9.84078   11.240591  11.007406  10.809069  11.072212\n",
      " 11.4622755 11.451399  10.990183  11.436956  11.369134  11.103796\n",
      " 11.167066  11.56533   11.355239  11.21841   11.539197  10.693672\n",
      " 11.44371   11.447181  10.743682  10.322756   7.947692  11.359251\n",
      " 11.355714  11.243334  11.445577  11.348817  11.550604  11.47388\n",
      "  8.691561  11.2727165 11.22273   10.562747  11.096861  11.615096\n",
      " 11.233035  11.453155  11.175482  11.888881  11.160945   9.804697\n",
      " 11.255906  11.317978  11.560407  11.613978  11.653684  11.459788\n",
      " 11.046742  11.368589  11.413346  10.519511  11.465631  11.570912\n",
      "  9.405233  11.393874 ], shape=(200,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for param in [model.bow_q1, model.bow_q2, model.bow_q3]:\n",
    "    print(tf.norm(param, axis=-1).numpy())\n",
    "print(model.distance_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
